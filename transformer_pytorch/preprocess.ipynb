{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368464ae-08cd-40c2-93dd-e4fad33f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill\n",
    "# !pip install spacy\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80902390-d4e5-48db-a8cd-9f0978e4c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? torchtext.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8279c8-cb8b-40d0-a730-2e9bafe410ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9988f9c5-4f4b-4186-95c4-e593a78398b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seugaoms/anaconda3/envs/python36/lib/python3.6/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TranslationDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dcbf98f79c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslationDataset\u001b[0m  \u001b[0;31m# ImportError: cannot import name 'TranslationDataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstants\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TranslationDataset'"
     ]
    }
   ],
   "source": [
    "''' Handling the data io '''\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import dill as pickle\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import codecs\n",
    "import spacy\n",
    "import torch\n",
    "import tarfile\n",
    "import torchtext.data\n",
    "\n",
    "import torchtext.datasets\n",
    "\n",
    "from torchtext.datasets import TranslationDataset  # ImportError: cannot import name 'TranslationDataset'\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "from learn_bpe import learn_bpe\n",
    "from apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3e7b4-1ea4-4de9-9248-b262671d81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Yu-Hsiang Huang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5304b0-59c2-49d5-a8df-576fe5892691",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRAIN_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/\" \\\n",
    "             \"training-parallel-nc-v12.tgz\",\n",
    "     \"trg\": \"news-commentary-v12.de-en.en\",\n",
    "     \"src\": \"news-commentary-v12.de-en.de\"},\n",
    "    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz\",\n",
    "    # \"trg\": \"commoncrawl.de-en.en\",\n",
    "    # \"src\": \"commoncrawl.de-en.de\"},\n",
    "    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\",\n",
    "    # \"trg\": \"europarl-v7.de-en.en\",\n",
    "    # \"src\": \"europarl-v7.de-en.de\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd32e35-3ed9-4a69-9140-2edf002f7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VAL_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/dev.tgz\",\n",
    "     \"trg\": \"newstest2013.en\",\n",
    "     \"src\": \"newstest2013.de\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dccb2-61c8-4711-b894-2fbb8a49689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_DATA_SOURCES = [\n",
    "    {\"url\": \"https://storage.googleapis.com/tf-perf-public/\" \\\n",
    "                \"official_transformer/test_data/newstest2014.tgz\",\n",
    "     \"trg\": \"newstest2014.en\",\n",
    "     \"src\": \"newstest2014.de\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456c388-a3ce-47fb-8006-b34550f7a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdc4bd-1696-45d1-bb78-988972caa1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exist(dir_name, file_name):\n",
    "    for sub_dir, _, files in os.walk(dir_name):\n",
    "        if file_name in files:\n",
    "            return os.path.join(sub_dir, file_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b330b-0d55-4c59-88e8-76e67bd54283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(download_dir, url, src_filename, trg_filename):\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    if src_path and trg_path:\n",
    "        sys.stderr.write(f\"Already downloaded and extracted {url}.\\n\")\n",
    "        return src_path, trg_path\n",
    "\n",
    "    compressed_file = _download_file(download_dir, url)\n",
    "\n",
    "    sys.stderr.write(f\"Extracting {compressed_file}.\\n\")\n",
    "    with tarfile.open(compressed_file, \"r:gz\") as corpus_tar:\n",
    "        corpus_tar.extractall(download_dir)\n",
    "\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    if src_path and trg_path:\n",
    "        return src_path, trg_path\n",
    "\n",
    "    raise OSError(f\"Download/extraction failed for url {url} to path {download_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edb7aa-a2a5-47c7-a8d3-49b6d87a4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_file(download_dir, url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if file_exist(download_dir, filename):\n",
    "        sys.stderr.write(f\"Already downloaded: {url} (at {filename}).\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(f\"Downloading from {url} to {filename}.\\n\")\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
    "            urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_raw_files(raw_dir, sources):\n",
    "    raw_files = { \"src\": [], \"trg\": [], }\n",
    "    for d in sources:\n",
    "        src_file, trg_file = download_and_extract(raw_dir, d[\"url\"], d[\"src\"], d[\"trg\"])\n",
    "        raw_files[\"src\"].append(src_file)\n",
    "        raw_files[\"trg\"].append(trg_file)\n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def mkdir_if_needed(dir_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52243e8-c2d6-43bc-92f5-5399978a4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_files(raw_dir, raw_files, prefix):\n",
    "    src_fpath = os.path.join(raw_dir, f\"raw-{prefix}.src\")\n",
    "    trg_fpath = os.path.join(raw_dir, f\"raw-{prefix}.trg\")\n",
    "\n",
    "    if os.path.isfile(src_fpath) and os.path.isfile(trg_fpath):\n",
    "        sys.stderr.write(f\"Merged files found, skip the merging process.\\n\")\n",
    "        return src_fpath, trg_fpath\n",
    "\n",
    "    sys.stderr.write(f\"Merge files into two files: {src_fpath} and {trg_fpath}.\\n\")\n",
    "\n",
    "    with open(src_fpath, 'w') as src_outf, open(trg_fpath, 'w') as trg_outf:\n",
    "        for src_inf, trg_inf in zip(raw_files['src'], raw_files['trg']):\n",
    "            sys.stderr.write(f'  Input files: \\n'\\\n",
    "                    f'    - SRC: {src_inf}, and\\n' \\\n",
    "                    f'    - TRG: {trg_inf}.\\n')\n",
    "            with open(src_inf, newline='\\n') as src_inf, open(trg_inf, newline='\\n') as trg_inf:\n",
    "                cntr = 0\n",
    "                for i, line in enumerate(src_inf):\n",
    "                    cntr += 1\n",
    "                    src_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "                for j, line in enumerate(trg_inf):\n",
    "                    cntr -= 1\n",
    "                    trg_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "                assert cntr == 0, 'Number of lines in two files are inconsistent.'\n",
    "    return src_fpath, trg_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d7a8e-fe22-4780-85a7-6ed206e62a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_file(bpe, in_file, out_file):\n",
    "    sys.stderr.write(f\"Read raw content from {in_file} and \\n\"\\\n",
    "            f\"Write encoded content to {out_file}\\n\")\n",
    "    \n",
    "    with codecs.open(in_file, encoding='utf-8') as in_f:\n",
    "        with codecs.open(out_file, 'w', encoding='utf-8') as out_f:\n",
    "            for line in in_f:\n",
    "                out_f.write(bpe.process_line(line))\n",
    "\n",
    "\n",
    "def encode_files(bpe, src_in_file, trg_in_file, data_dir, prefix):\n",
    "    src_out_file = os.path.join(data_dir, f\"{prefix}.src\")\n",
    "    trg_out_file = os.path.join(data_dir, f\"{prefix}.trg\")\n",
    "\n",
    "    if os.path.isfile(src_out_file) and os.path.isfile(trg_out_file):\n",
    "        sys.stderr.write(f\"Encoded files found, skip the encoding process ...\\n\")\n",
    "\n",
    "    encode_file(bpe, src_in_file, src_out_file)\n",
    "    encode_file(bpe, trg_in_file, trg_out_file)\n",
    "    return src_out_file, trg_out_file\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-raw_dir', required=True)\n",
    "    parser.add_argument('-data_dir', required=True)\n",
    "    parser.add_argument('-codes', required=True)\n",
    "    parser.add_argument('-save_data', required=True)\n",
    "    parser.add_argument('-prefix', required=True)\n",
    "    parser.add_argument('-max_len', type=int, default=100)\n",
    "    parser.add_argument('--symbols', '-s', type=int, default=32000, help=\"Vocabulary size\")\n",
    "    parser.add_argument(\n",
    "        '--min-frequency', type=int, default=6, metavar='FREQ',\n",
    "        help='Stop if no symbol pair has frequency >= FREQ (default: %(default)s))')\n",
    "    parser.add_argument('--dict-input', action=\"store_true\",\n",
    "        help=\"If set, input file is interpreted as a dictionary where each line contains a word-count pair\")\n",
    "    parser.add_argument(\n",
    "        '--separator', type=str, default='@@', metavar='STR',\n",
    "        help=\"Separator between non-final subword units (default: '%(default)s'))\")\n",
    "    parser.add_argument('--total-symbols', '-t', action=\"store_true\")\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # Create folder if needed.\n",
    "    mkdir_if_needed(opt.raw_dir)\n",
    "    mkdir_if_needed(opt.data_dir)\n",
    "\n",
    "    # Download and extract raw data.\n",
    "    raw_train = get_raw_files(opt.raw_dir, _TRAIN_DATA_SOURCES)\n",
    "    raw_val = get_raw_files(opt.raw_dir, _VAL_DATA_SOURCES)\n",
    "    raw_test = get_raw_files(opt.raw_dir, _TEST_DATA_SOURCES)\n",
    "\n",
    "    # Merge files into one.\n",
    "    train_src, train_trg = compile_files(opt.raw_dir, raw_train, opt.prefix + '-train')\n",
    "    val_src, val_trg = compile_files(opt.raw_dir, raw_val, opt.prefix + '-val')\n",
    "    test_src, test_trg = compile_files(opt.raw_dir, raw_test, opt.prefix + '-test')\n",
    "\n",
    "    # Build up the code from training files if not exist\n",
    "    opt.codes = os.path.join(opt.data_dir, opt.codes)\n",
    "    if not os.path.isfile(opt.codes):\n",
    "        sys.stderr.write(f\"Collect codes from training data and save to {opt.codes}.\\n\")\n",
    "        learn_bpe(raw_train['src'] + raw_train['trg'], opt.codes, opt.symbols, opt.min_frequency, True)\n",
    "    sys.stderr.write(f\"BPE codes prepared.\\n\")\n",
    "\n",
    "    sys.stderr.write(f\"Build up the tokenizer.\\n\")\n",
    "    with codecs.open(opt.codes, encoding='utf-8') as codes: \n",
    "        bpe = BPE(codes, separator=opt.separator)\n",
    "\n",
    "    sys.stderr.write(f\"Encoding ...\\n\")\n",
    "    encode_files(bpe, train_src, train_trg, opt.data_dir, opt.prefix + '-train')\n",
    "    encode_files(bpe, val_src, val_trg, opt.data_dir, opt.prefix + '-val')\n",
    "    encode_files(bpe, test_src, test_trg, opt.data_dir, opt.prefix + '-test')\n",
    "    sys.stderr.write(f\"Done.\\n\")\n",
    "\n",
    "\n",
    "    field = torchtext.data.Field(\n",
    "        tokenize=str.split,\n",
    "        lower=True,\n",
    "        pad_token=Constants.PAD_WORD,\n",
    "        init_token=Constants.BOS_WORD,\n",
    "        eos_token=Constants.EOS_WORD)\n",
    "\n",
    "    fields = (field, field)\n",
    "\n",
    "    MAX_LEN = opt.max_len\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    enc_train_files_prefix = opt.prefix + '-train'\n",
    "    train = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=os.path.join(opt.data_dir, enc_train_files_prefix),\n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "\n",
    "    from itertools import chain\n",
    "    field.build_vocab(chain(train.src, train.trg), min_freq=2)\n",
    "\n",
    "    data = { 'settings': opt, 'vocab': field, }\n",
    "    opt.save_data = os.path.join(opt.data_dir, opt.save_data)\n",
    "\n",
    "    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62014e46-9cf7-4c0d-889c-0adbd3cbc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_wo_bpe():\n",
    "    '''\n",
    "    Usage: python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n",
    "    '''\n",
    "\n",
    "    spacy_support_langs = ['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt']\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-lang_src', required=True, choices=spacy_support_langs)\n",
    "    parser.add_argument('-lang_trg', required=True, choices=spacy_support_langs)\n",
    "    parser.add_argument('-save_data', required=True)\n",
    "    parser.add_argument('-data_src', type=str, default=None)\n",
    "    parser.add_argument('-data_trg', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('-max_len', type=int, default=100)\n",
    "    parser.add_argument('-min_word_count', type=int, default=3)\n",
    "    parser.add_argument('-keep_case', action='store_true')\n",
    "    parser.add_argument('-share_vocab', action='store_true')\n",
    "    #parser.add_argument('-ratio', '--train_valid_test_ratio', type=int, nargs=3, metavar=(8,1,1))\n",
    "    #parser.add_argument('-vocab', default=None)\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    assert not any([opt.data_src, opt.data_trg]), 'Custom data input is not support now.'\n",
    "    assert not any([opt.data_src, opt.data_trg]) or all([opt.data_src, opt.data_trg])\n",
    "    print(opt)\n",
    "\n",
    "    src_lang_model = spacy.load(opt.lang_src)\n",
    "    trg_lang_model = spacy.load(opt.lang_trg)\n",
    "\n",
    "    def tokenize_src(text):\n",
    "        return [tok.text for tok in src_lang_model.tokenizer(text)]\n",
    "\n",
    "    def tokenize_trg(text):\n",
    "        return [tok.text for tok in trg_lang_model.tokenizer(text)]\n",
    "\n",
    "    SRC = torchtext.data.Field(\n",
    "        tokenize=tokenize_src, lower=not opt.keep_case,\n",
    "        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n",
    "\n",
    "    TRG = torchtext.data.Field(\n",
    "        tokenize=tokenize_trg, lower=not opt.keep_case,\n",
    "        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n",
    "\n",
    "    MAX_LEN = opt.max_len\n",
    "    MIN_FREQ = opt.min_word_count\n",
    "\n",
    "    if not all([opt.data_src, opt.data_trg]):\n",
    "        assert {opt.lang_src, opt.lang_trg} == {'de', 'en'}\n",
    "    else:\n",
    "        # Pack custom txt file into example datasets\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train, val, test = torchtext.datasets.Multi30k.splits(\n",
    "            exts = ('.' + opt.lang_src, '.' + opt.lang_trg),\n",
    "            fields = (SRC, TRG),\n",
    "            filter_pred=filter_examples_with_length)\n",
    "\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    print('[Info] Get source language vocabulary size:', len(SRC.vocab))\n",
    "    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "    print('[Info] Get target language vocabulary size:', len(TRG.vocab))\n",
    "\n",
    "    if opt.share_vocab:\n",
    "        print('[Info] Merging two vocabulary ...')\n",
    "        for w, _ in SRC.vocab.stoi.items():\n",
    "            # TODO: Also update the `freq`, although it is not likely to be used.\n",
    "            if w not in TRG.vocab.stoi:\n",
    "                TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n",
    "        TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n",
    "        for w, i in TRG.vocab.stoi.items():\n",
    "            TRG.vocab.itos[i] = w\n",
    "        SRC.vocab.stoi = TRG.vocab.stoi\n",
    "        SRC.vocab.itos = TRG.vocab.itos\n",
    "        print('[Info] Get merged vocabulary size:', len(TRG.vocab))\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'settings': opt,\n",
    "        'vocab': {'src': SRC, 'trg': TRG},\n",
    "        'train': train.examples,\n",
    "        'valid': val.examples,\n",
    "        'test': test.examples}\n",
    "\n",
    "    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbdcad-d7f5-4653-b6fe-01e75b66f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_wo_bpe()\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
